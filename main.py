import torch.nn as nn
import torch.optim as optim
import os
import nltk
from torch.utils.data import DataLoader
import sys
import numpy as np

import config
import data_handler
from data_handler import load_label_map, Vocabulary
import models
import engine

PAD_IDX = config.PAD_IDX

def check_nltk_resource(resource_id, resource_name):
    try:
        nltk.data.find(f'corpora/{resource_id}')
        print(f"NLTK resource '{resource_name}' found.")
    except LookupError:
        print(f"NLTK resource '{resource_name}' not found. Downloading...")
        try:
            nltk.download(resource_id, quiet=True)
            print(f"NLTK resource '{resource_name}' downloaded successfully.")
        except Exception as e:
            print(f"Error downloading NLTK resource '{resource_name}': {e}")
            print(f"Please try downloading it manually: python -m nltk.downloader {resource_id}")

def run_training():
    print("--- Starting Emotion Classification Training ---")
    print(f"Using device: {config.DEVICE}")
    print(f"Selected model type: {config.MODEL_TYPE}")

    os.makedirs(config.ARTIFACTS_DIR, exist_ok=True)
    check_nltk_resource('stopwords', 'stopwords')
    check_nltk_resource('wordnet', 'wordnet')

    try:
        print("Loading data, handling labels, and creating/saving label map...")
        train_df, val_df, test_df, label_to_int, int_to_label_map, initial_n_class = data_handler.load_and_prepare_data(
            config.TRAIN_PATH, config.VAL_PATH, config.TEST_PATH, config.LABEL_MAP_PATH
        )
        print(f"Data loading complete. Initial n_class from data: {initial_n_class}")
    except FileNotFoundError as e:
        print(f"Error: Data file not found: {e}. Please check paths in config.py")
        sys.exit(1)
    except (TypeError, ValueError, RuntimeError) as e:
         print(f"Error during data loading or label processing: {e}")
         import traceback
         traceback.print_exc()
         sys.exit(1)
    except Exception as e:
        print(f"An unexpected error occurred during data loading: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

    if int_to_label_map is None:
         print("Error: int_to_label_map was not generated by load_and_prepare_data. Exiting.")
         sys.exit(1)

    print("\n--- Data Analysis ---")
    data_handler.plot_class_distribution(
        train_df, label_column='label', title="Training Set Class Distribution",
        save_path=os.path.join(config.ARTIFACTS_DIR, "train_class_distribution.png"),
        int_to_label_map=int_to_label_map
    )

    print("\nInitializing TextPreprocessor...")
    text_preprocessor = data_handler.TextPreprocessor(use_stopwords=True)

    print("Preprocessing training data (needed for sequence length analysis and potential vocab building)...")
    train_tokens_list = text_preprocessor.preprocess_dataframe(train_df)

    data_handler.plot_sequence_lengths(
        train_tokens_list, title="Training Sequence Length Distribution (After Tokenization)",
        save_path=os.path.join(config.ARTIFACTS_DIR, "train_seq_length_distribution.png")
    )

    vocabulary = None
    n_class = None

    print("\nChecking for existing Vocabulary...")
    if os.path.exists(config.VOCAB_SAVE_PATH):
        print(f"Found existing vocabulary file: {config.VOCAB_SAVE_PATH}")
        try:
            vocabulary, loaded_n_class = Vocabulary.load(config.VOCAB_SAVE_PATH)
            print(f"Successfully loaded vocabulary. Size: {len(vocabulary)}, n_class from file: {loaded_n_class}")
            n_class = loaded_n_class

            if n_class != initial_n_class:
                print(f"Warning: Loaded n_class ({n_class}) differs from n_class derived "
                      f"from current training data ({initial_n_class}). "
                      f"Using n_class from the loaded vocabulary file.")

        except Exception as e:
            print(f"Error loading existing vocabulary from {config.VOCAB_SAVE_PATH}: {e}")
            print("Cannot proceed with potentially corrupted vocabulary artifact.")
            print(f"Please remove or fix the file: {config.VOCAB_SAVE_PATH}")
            sys.exit(1)
    else:
        print(f"No vocabulary file found at {config.VOCAB_SAVE_PATH}. Building a new one...")
        vocabulary = data_handler.Vocabulary(freq_threshold=config.MIN_FREQ)
        vocabulary.build_vocabulary(train_tokens_list)
        n_class = initial_n_class
        try:
            vocabulary.save(config.VOCAB_SAVE_PATH, n_class=n_class)
            print(f"New vocabulary built and saved. Size: {len(vocabulary)}, n_class: {n_class}")
        except Exception as e:
             print(f"Error saving newly built vocabulary to {config.VOCAB_SAVE_PATH}: {e}")
             sys.exit(1)

    if vocabulary is None or n_class is None:
        print("Error: Vocabulary or n_class could not be determined. Exiting.")
        sys.exit(1)

    vocab_size = len(vocabulary)
    print(f"\nUsing vocabulary size: {vocab_size}")
    print(f"Using number of classes (n_class): {n_class}")

    print("\nNumericalizing datasets using loaded/built vocabulary...")
    def numericalize_tokens(tokens_list, vocab, max_len):
        numericalized = []
        num_truncated = 0
        for tokens in tokens_list:
             if len(tokens) > max_len - 2:
                 num_truncated += 1
             truncated_tokens = tokens[:max_len-2]
             seq = [config.SOS_IDX] + vocab.numericalize(truncated_tokens) + [config.EOS_IDX]
             numericalized.append(seq)
        if num_truncated > 0:
             print(f"Info: Truncated {num_truncated}/{len(tokens_list)} sequences to max content length {max_len-2}.")
        return numericalized

    train_sequences = numericalize_tokens(train_tokens_list, vocabulary, config.MAX_LENGTH)

    print("Preprocessing and numericalizing validation data...")
    val_tokens_list = text_preprocessor.preprocess_dataframe(val_df)
    val_sequences = numericalize_tokens(val_tokens_list, vocabulary, config.MAX_LENGTH)

    print("Preprocessing and numericalizing test data...")
    test_tokens_list = text_preprocessor.preprocess_dataframe(test_df)
    test_sequences = numericalize_tokens(test_tokens_list, vocabulary, config.MAX_LENGTH)

    train_labels = train_df['label'].to_numpy(dtype=np.int64)
    val_labels = val_df['label'].to_numpy(dtype=np.int64)
    test_labels = test_df['label'].to_numpy(dtype=np.int64)

    print("\nCreating PyTorch Datasets...")
    train_dataset = data_handler.EmotionDataset(train_sequences, train_labels)
    val_dataset = data_handler.EmotionDataset(val_sequences, val_labels)
    test_dataset = data_handler.EmotionDataset(test_sequences, test_labels)

    print("Creating DataLoaders...")
    pin_memory = config.DEVICE == "cuda"
    num_workers = config.NUM_WORKERS if config.NUM_WORKERS > 0 else 0
    if os.name == 'nt' and num_workers > 0:
        num_workers = 0

    train_loader = DataLoader(
        dataset=train_dataset, batch_size=config.BATCH_SIZE,
        shuffle=config.SHUFFLE_DATA, collate_fn=data_handler.collate_batch,
        num_workers=num_workers, pin_memory=pin_memory, persistent_workers=num_workers > 0
    )
    val_loader = DataLoader(
        dataset=val_dataset, batch_size=config.BATCH_SIZE,
        shuffle=False, collate_fn=data_handler.collate_batch,
        num_workers=num_workers, pin_memory=pin_memory, persistent_workers=num_workers > 0
    )
    test_loader = DataLoader(
        dataset=test_dataset, batch_size=config.BATCH_SIZE,
        shuffle=False, collate_fn=data_handler.collate_batch,
        num_workers=num_workers, pin_memory=pin_memory, persistent_workers=num_workers > 0
    )

    print(f"\nBuilding model: {config.MODEL_TYPE}")
    if config.MODEL_TYPE == 'LSTM':
        model = models.LSTMNetwork(
            vocab_size=vocab_size,
            embedding_dim=config.EMBEDDING_DIM,
            hidden_dim=config.HIDDEN_DIM,
            n_class=n_class,
            n_layers=config.N_LAYERS,
            pad_idx=PAD_IDX,
            dropout_prob=config.DROPOUT_PROB
        )
    else:
        raise ValueError(f"Unsupported model type: {config.MODEL_TYPE}")

    model.to(config.DEVICE)

    optimizer = optim.AdamW(model.parameters(), lr=config.LEARNING_RATE_LSTM, weight_decay=config.WEIGHT_DECAY)
    criterion = nn.CrossEntropyLoss()

    print(f"Model:\n{model}")
    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"Total Trainable Parameters: {total_params:,}")
    print(f"Optimizer: {optimizer}")
    print(f"Criterion: {criterion}")

    print("\nStarting training...")
    trained_model, history_df = engine.trainer(
        model=model,
        train_loader=train_loader,
        optimizer=optimizer,
        criterion=criterion,
        epochs=config.EPOCHS,
        device=config.DEVICE,
        val_loader=val_loader,
        model_save_path=config.MODEL_SAVE_PATH,
    )

    print("\nPlotting training history...")
    engine.plot_history(history_df, config.RESULTS_PLOT_PATH)

    print("\nEvaluating final model on test set...")
    if int_to_label_map is None:
        print("Error: Cannot generate test report because int_to_label_map is missing.")
    else:
        print("Generating test report using the loaded/created label map...")
        engine.generate_test_report(
            model=trained_model,
            data_loader=test_loader,
            criterion=criterion,
            device=config.DEVICE,
            int_to_label_map=int_to_label_map,
            report_save_path=os.path.join(config.ARTIFACTS_DIR, "test_classification_report.txt"),
            conf_matrix_save_path=os.path.join(config.ARTIFACTS_DIR, "test_confusion_matrix.png")
        )

    print("\n--- Training Pipeline Finished ---")

if __name__ == "__main__":
    if not hasattr(config, 'NUM_WORKERS'):
        config.NUM_WORKERS = 0
        print("Setting config.NUM_WORKERS to default 0")
    if not hasattr(config, 'DROPOUT_PROB'):
        config.DROPOUT_PROB = 0.5
        print(f"Setting config.DROPOUT_PROB to default {config.DROPOUT_PROB}")
    if not hasattr(config, 'WEIGHT_DECAY'):
        config.WEIGHT_DECAY = 0.0
        print(f"Setting config.WEIGHT_DECAY to default {config.WEIGHT_DECAY}")

    run_training()